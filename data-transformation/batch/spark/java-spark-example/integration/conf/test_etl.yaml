version: 1.0.0
job:
    sparkConf:
        appName: "test_etl"
        master: "local[*]"
        conf:
            spark.sql.shuffle.partitions: 10
            spark.sql.autoBroadcastJoinThreshold: -1
            hive.metadata.warehouse.dir: "hdfs://localhost:9000/user/hive/warehouse"
    sourceConf:
          name: "test"
          format: csv
          path: "/Users/tuan.nguyen3/Documents/Personal-Projects/spark-etl-tool/integration/data/test.csv"
          schema: "id int, name string"
    transformConf:
        - type: filter
          condition: "col('id') == 1"
        - type: select
          columns: ["id", "name"]
        - type: map
          function: "lambda x: x + 1"
    sinkConf:
        connection: "gcs://test/output.parquet"
        format: parquet